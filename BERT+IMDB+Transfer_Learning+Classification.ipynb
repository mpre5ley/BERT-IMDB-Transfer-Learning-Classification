{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpre5ley/BERT-IMDB-Transfer-Learning-Classification/blob/main/BERT%2BIMDB%2BTransfer_Learning%2BClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snbcxTAGxRh0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAUe73IV5fLT"
      },
      "outputs": [],
      "source": [
        "imdb_train, ds_info=tfds.load(name=\"imdb_reviews\", split=\"train\", with_info=True, as_supervised=True)\n",
        "imdb_test=tfds.load(name=\"imdb_reviews\", split=\"test\", as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rkpgEKy7b0W",
        "outputId": "1d9db459-83fe-4bb5-e699-007ef0a7bd44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='imdb_reviews',\n",
              "    full_name='imdb_reviews/plain_text/1.0.0',\n",
              "    description=\"\"\"\n",
              "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
              "    classification containing substantially more data than previous benchmark\n",
              "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
              "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
              "    \"\"\",\n",
              "    config_description=\"\"\"\n",
              "    Plain text\n",
              "    \"\"\",\n",
              "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
              "    data_path='/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
              "    file_format=tfrecord,\n",
              "    download_size=80.23 MiB,\n",
              "    dataset_size=129.83 MiB,\n",
              "    features=FeaturesDict({\n",
              "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
              "        'text': Text(shape=(), dtype=string),\n",
              "    }),\n",
              "    supervised_keys=('text', 'label'),\n",
              "    disable_shuffling=False,\n",
              "    splits={\n",
              "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
              "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
              "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
              "    },\n",
              "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
              "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
              "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
              "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
              "      month     = {June},\n",
              "      year      = {2011},\n",
              "      address   = {Portland, Oregon, USA},\n",
              "      publisher = {Association for Computational Linguistics},\n",
              "      pages     = {142--150},\n",
              "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
              "    }\"\"\",\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "ds_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuofPHQOHaiK"
      },
      "outputs": [],
      "source": [
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "vocabulary_set = set()\n",
        "MAX_TOKENS = 0\n",
        "for example, label in imdb_train:\n",
        "  some_tokens = tokenizer.tokenize(example.numpy())\n",
        "  if MAX_TOKENS < len(some_tokens):\n",
        "            MAX_TOKENS = len(some_tokens)\n",
        "  vocabulary_set.update(some_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE_a3KTcIHq6",
        "outputId": "ac55f9b7-b455-497b-8164-f87d006dc1f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93929"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(vocabulary_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjKsjM3kKKmC",
        "outputId": "8b73b346-a781-4798-cc37-1794ae662e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93931 2525\n"
          ]
        }
      ],
      "source": [
        "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, lowercase=True,tokenizer=tokenizer)\n",
        "vocab_size = imdb_encoder.vocab_size\n",
        "print(vocab_size, MAX_TOKENS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2UVq9oP_Pz3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "def encode_pad_transform(sample):\n",
        "    encoded = imdb_encoder.encode(sample.numpy())\n",
        "    pad = sequence.pad_sequences([encoded], padding='post',\n",
        "                                 maxlen=150)\n",
        "    return np.array(pad[0], dtype=np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo91gGiHUf9g"
      },
      "outputs": [],
      "source": [
        "def encode_tf_fn(sample, label):\n",
        "    encoded = tf.py_function(encode_pad_transform,\n",
        "                             inp=[sample],\n",
        "                             Tout=(tf.int64))\n",
        "    encoded.set_shape([None])\n",
        "    label.set_shape([])\n",
        "    return encoded, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJMSDrSdVJRF"
      },
      "outputs": [],
      "source": [
        "encoded_train = imdb_train.map(encode_tf_fn,\n",
        "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "encoded_test = imdb_test.map(encode_tf_fn,\n",
        "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31ou8iLi6lwC",
        "outputId": "1ba643a6-30e4-4813-b416-dae0c01fb20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ParallelMapDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDFnjs57Xn8v"
      },
      "outputs": [],
      "source": [
        "path=\"/content/drive/MyDrive/Chatbots/glove.6B/glove.6B.50d.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "vfFJwEebYWUh",
        "outputId": "cd71a3b7-727b-414b-834f-e6f847db4fab"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5ff704e302d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdict_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Chatbots/glove.6B/glove.6B.50d.txt'"
          ]
        }
      ],
      "source": [
        "dict_w2v = {}\n",
        "with open(path, \"r\") as file:\n",
        "    for line in file:\n",
        "      tokens = line.split()\n",
        "      word = tokens[0]\n",
        "      vector = np.array(tokens[1:], dtype=np.float32)\n",
        "      if vector.shape[0] == 50:\n",
        "          dict_w2v[word] = vector\n",
        "      else:\n",
        "            print(\"There was an issue with \" + word)\n",
        "# let's check the vocabulary size\n",
        "print(\"Dictionary Size: \", len(dict_w2v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6VqBi2NWyk9"
      },
      "outputs": [],
      "source": [
        "dict_w2v.get(\"good\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfqEGMKxXrI8"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 50\n",
        "embedding_matrix = np.zeros((imdb_encoder.vocab_size, embedding_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF9PQQ5OXtAz"
      },
      "outputs": [],
      "source": [
        "imdb_encoder.encode(\"good\")[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwLiCPUWYueJ"
      },
      "outputs": [],
      "source": [
        "len(imdb_encoder.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRH0H0b_dnVq"
      },
      "outputs": [],
      "source": [
        "unk_cnt = 0\n",
        "unk_set = set()\n",
        "for word in imdb_encoder.tokens:\n",
        "    embedding_vector = dict_w2v.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        tkn_id = imdb_encoder.encode(word)[0]\n",
        "        embedding_matrix[tkn_id] = embedding_vector\n",
        "    else:\n",
        "        unk_cnt += 1\n",
        "        unk_set.add(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGJspC9Md27C"
      },
      "outputs": [],
      "source": [
        "print(unk_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKhoKlj8fDux"
      },
      "source": [
        "During the data loading step, we saw that the total number of tokens was 93,931. Out of these, 14,553 words could not be found, which is approximately 15% of the tokens. For these words, the embedding matrix will have zeros. This is the first step in transfer learning. Now that the setup is completed, we will need to use TensorFlow to use these pre-trained embeddings. There will be two different models that will be tried – the first will be based on feature extraction and the second one on fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
        "# Number of RNN units\n",
        "rnn_units = 64\n",
        "#batch size\n",
        "BATCH_SIZE=100"
      ],
      "metadata": {
        "id": "BweiM_Ug0bd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense"
      ],
      "metadata": {
        "id": "tJfnPAwy06Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_bilstm(vocab_size, embedding_dim,rnn_units, batch_size, train_emb=False):\n",
        "  model = tf.keras.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, mask_zero=True, weights=[embedding_matrix], trainable=train_emb),\n",
        "    Bidirectional(LSTM(rnn_units, return_sequences=True,dropout=0.5)),\n",
        "    Bidirectional(LSTM(rnn_units,dropout=0.25)),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "CezADmr41HSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe = build_model_bilstm(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  train_emb=True)"
      ],
      "metadata": {
        "id": "OBj97aHI5tRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe.summary()"
      ],
      "metadata": {
        "id": "32MhL3WH6Dz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe.compile(loss='binary_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy', 'Precision', 'Recall'])"
      ],
      "metadata": {
        "id": "CyT8F-Xl6NTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
      ],
      "metadata": {
        "id": "a80IpLSw6UL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe.fit(encoded_train_batched, epochs=10)"
      ],
      "metadata": {
        "id": "307Ic-Gt7t5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe.evaluate(encoded_test.batch(BATCH_SIZE))"
      ],
      "metadata": {
        "id": "7ScCyFF7PlEd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}